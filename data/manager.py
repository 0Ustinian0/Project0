"""æ•°æ®åŠ è½½ï¼šä» CSV ç›®å½•åŠ è½½å¹¶ç”Ÿæˆ Backtrader feeds"""
import os
import pandas as pd
import backtrader as bt


def validate_data(df, strict=True):
    """
    æ•°æ®éªŒè¯å±‚ï¼šæ£€æŸ¥ç¼ºå¤±å€¼ã€é€»è¾‘é”™è¯¯ï¼ˆHigh < Lowï¼‰ã€åœç‰Œï¼ˆæˆäº¤é‡ä¸º 0ï¼‰ç­‰ã€‚
    strict=True æ—¶å‘ç°ä¸¥é‡é”™è¯¯ä¼š raiseï¼›å¦åˆ™ä»…æ‰“å°è­¦å‘Šã€‚
    """
    if df is None or df.empty:
        raise ValueError("æ•°æ®ä¸ºç©º")

    # æ£€æŸ¥ç¼ºå¤±å€¼
    if df.isnull().values.any():
        msg = "è­¦å‘Šï¼šå‘ç°ç¼ºå¤±æ•°æ®"
        if strict:
            raise ValueError(msg)
        if hasattr(validate_data, "_warned_null"):
            pass
        else:
            print(msg)

    # å¿…éœ€åˆ—
    required = ['Open', 'High', 'Low', 'Close', 'Volume']
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing}")

    # æ£€æŸ¥é€»è¾‘é”™è¯¯ï¼šHigh < Low
    if (df['High'] < df['Low']).any():
        raise ValueError("æ•°æ®é”™è¯¯ï¼šå­˜åœ¨ High < Low çš„è¡Œæƒ…")

    # æ£€æŸ¥ Close æ˜¯å¦åœ¨ [Low, High] å†…
    if (df['Close'] > df['High']).any() or (df['Close'] < df['Low']).any():
        raise ValueError("æ•°æ®é”™è¯¯ï¼šClose è¶…å‡º High/Low èŒƒå›´")

    # æ£€æŸ¥åœç‰Œï¼ˆæˆäº¤é‡ä¸º 0ï¼‰ï¼šä»…è­¦å‘Š
    zero_vol = (df['Volume'] == 0).sum()
    if zero_vol > 0:
        print(f"è­¦å‘Šï¼šå‘ç° {zero_vol} è¡Œæˆäº¤é‡ä¸º 0ï¼ˆå¯èƒ½åœç‰Œï¼‰")

    # æ£€æŸ¥å…¨ 0 è¡Œï¼ˆæŸå¤©æ•°æ®å…¨ä¸º 0ï¼‰
    ohlc_zero = ((df['Open'] == 0) & (df['High'] == 0) & (df['Low'] == 0) & (df['Close'] == 0))
    if ohlc_zero.any():
        if strict:
            raise ValueError("æ•°æ®é”™è¯¯ï¼šå­˜åœ¨ OHLC å…¨ä¸º 0 çš„è¡Œæƒ…")
        print("è­¦å‘Šï¼šå­˜åœ¨ OHLC å…¨ä¸º 0 çš„è¡Œæƒ…")

    return True


def add_csv_feed(cerebro, filepath, name, start, end, min_bars=None, logger=None):
    """
    è¯»å–å•åªè‚¡ç¥¨ CSVï¼Œè½¬æ¢ä¸º PandasData å¹¶åŠ å…¥ cerebroã€‚
    å…¼å®¹æ ¼å¼ï¼šskiprows=3, åˆ—ä¸º Date, Close, High, Low, Open, Volumeã€‚
    min_bars: è‹¥è®¾ç½®ï¼Œçª—å£å†… K çº¿æ•°å°‘äºæ­¤æ•°åˆ™ä¸åŠ è½½ï¼ˆç”¨äº WFA ç­‰é¿å… SMA200 ç­‰è¶Šç•Œï¼‰ã€‚
    """
    try:
        df = pd.read_csv(
            filepath,
            skiprows=3,
            header=None,
            names=['Date', 'Close', 'High', 'Low', 'Open', 'Volume'],
            parse_dates=[0],
            index_col=0
        )
        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])
        df = df[(df.index >= pd.Timestamp(start)) & (df.index <= pd.Timestamp(end))]
        if len(df) == 0:
            return False
        if min_bars is not None and len(df) < min_bars:
            return False
        validate_data(df, strict=True)
        data = bt.feeds.PandasData(
            dataname=df,
            name=name,
            fromdate=start,
            todate=end,
            open='Open', high='High', low='Low', close='Close', volume='Volume',
            openinterest=None
        )
        cerebro.adddata(data)
        return True
    except Exception as e:
        if logger:
            logger.warning(f"åŠ è½½ {name} å¤±è´¥: {e}")
        return False


def load_data_into_cerebro(cerebro, data_dir, from_date, to_date, universe_size=None, universe_seed=None, min_bars=None, logger=None):
    """
    å°† data_dir ä¸‹çš„ CSV åŠ è½½åˆ° cerebroï¼šSPY ä½œä¸º data0ï¼Œå…¶ä½™æŒ‰ universe_size é™åˆ¶æ•°é‡ã€‚
    min_bars: è‹¥è®¾ç½®ï¼Œçª—å£å†… K çº¿æ•°å°‘äºæ­¤æ•°çš„æ ‡çš„ä¸åŠ è½½ï¼ˆWFA ç­‰éœ€è‡³å°‘ 252 æ ¹ K çº¿æ—¶è®¾ 252ï¼‰ã€‚
    """
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    all_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.csv')])
    if 'SPY.csv' in all_files:
        add_csv_feed(cerebro, os.path.join(data_dir, 'SPY.csv'), 'SPY', from_date, to_date, min_bars=min_bars, logger=logger)
        all_files.remove('SPY.csv')
    else:
        if logger:
            logger.warning("æœªæ‰¾åˆ° SPY.csvï¼Œå¤§ç›˜é£æ§å¯èƒ½å¤±æ•ˆ")
    if universe_size is not None and universe_size > 0:
        if universe_seed is not None:
            import random
            rng = random.Random(universe_seed)
            all_files = all_files.copy()
            rng.shuffle(all_files)
        target_files = all_files[:universe_size]
    else:
        target_files = all_files
    for filename in target_files:
        ticker = filename.split('.')[0]
        filepath = os.path.join(data_dir, filename)
        add_csv_feed(cerebro, filepath, ticker, from_date, to_date, min_bars=min_bars, logger=logger)
    if logger:
        logger.info(f"ğŸ“Š [æ•°æ®] è£…è½½å®Œæˆã€‚æ€»è®¡: {len(cerebro.datas)} åª (å«SPY)")
    return len(cerebro.datas)


def load_fundamentals(data_dir, logger=None):
    """
    ä» data_dir/fundamentals.csv åŠ è½½åŸºæœ¬é¢æ•°æ®ï¼ˆå¯é€‰ï¼‰ã€‚
    CSV æ ¼å¼ï¼šTicker, PE, PB, ROE, RevenueGrowth, DebtToEquity, Sector, EPS_Growth, MarketCap
    - PE/PB: å¸‚ç›ˆç‡/å¸‚å‡€ç‡ï¼Œç©ºæˆ–è´Ÿè¡¨ç¤ºäºæŸæˆ–æ— æ•ˆ
    - ROE/RevenueGrowth: å°æ•°å½¢å¼ï¼Œå¦‚ 0.15 è¡¨ç¤º 15%
    - DebtToEquity: è´Ÿå€º/æƒç›Š
    - Sector: æ¿å—åç§°ï¼ˆå¦‚ Technologyï¼‰ï¼Œç”¨äº filter_sector
    - EPS_Growth: ç›ˆåˆ©å¢é•¿ï¼Œæ”¯æŒç™¾åˆ†æ•° 15 æˆ–å°æ•° 0.15 è¡¨ç¤º 15%
    è¿”å›: DataFrame index=Tickerï¼Œåˆ—å« PE/PB/ROE/RevenueGrowth/DebtToEquity/Sector/EPS_Growth ç­‰ï¼›æ— æ–‡ä»¶è¿”å› Noneã€‚
    """
    if not data_dir or not os.path.exists(data_dir):
        return None
    path = os.path.join(data_dir, 'fundamentals.csv')
    if not os.path.exists(path):
        return None
    try:
        df = pd.read_csv(path, dtype=str)
        df = df.rename(columns=lambda c: c.strip().lower().replace(' ', '_'))
        col_map = {'ticker': 'Ticker', 'pe': 'PE', 'pb': 'PB', 'roe': 'ROE',
                   'revenue_growth': 'RevenueGrowth', 'revenuegrowth': 'RevenueGrowth',
                   'debt_to_equity': 'DebtToEquity', 'debttoequity': 'DebtToEquity',
                   'sector': 'Sector', 'eps_growth': 'EPS_Growth', 'epsgrowth': 'EPS_Growth',
                   'marketcap': 'MarketCap', 'market_cap': 'MarketCap'}
        for k, v in col_map.items():
            if k in df.columns and v not in df.columns:
                df[v] = df[k]
        if 'Ticker' not in df.columns and 'ticker' in df.columns:
            df['Ticker'] = df['ticker']
        numeric_cols = ['PE', 'PB', 'ROE', 'RevenueGrowth', 'DebtToEquity', 'EPS_Growth']
        for c in numeric_cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors='coerce')
        if 'EPS_Growth' in df.columns:
            s = df['EPS_Growth'].dropna()
            if len(s) > 0 and s.abs().median() > 1:
                df['EPS_Growth'] = pd.to_numeric(df['EPS_Growth'], errors='coerce') / 100.0
        keep = [c for c in numeric_cols + ['Sector', 'MarketCap'] if c in df.columns]
        df = df.set_index('Ticker')
        df = df[[c for c in keep if c in df.columns]] if keep else df
        if logger:
            logger.debug(f"åŸºæœ¬é¢æ•°æ®å·²åŠ è½½: {path}, {len(df)} åª")
        return df
    except Exception as e:
        if logger:
            logger.warning(f"åŠ è½½åŸºæœ¬é¢æ–‡ä»¶å¤±è´¥ {path}: {e}")
        return None
