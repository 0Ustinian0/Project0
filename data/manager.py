"""æ•°æ®åŠ è½½ï¼šä» CSV ç›®å½•åŠ è½½å¹¶ç”Ÿæˆ Backtrader feeds"""
import os
import pandas as pd
import backtrader as bt


def validate_data(df, strict=True):
    """
    æ•°æ®éªŒè¯å±‚ï¼šæ£€æŸ¥ç¼ºå¤±å€¼ã€é€»è¾‘é”™è¯¯ï¼ˆHigh < Lowï¼‰ã€åœç‰Œï¼ˆæˆäº¤é‡ä¸º 0ï¼‰ç­‰ã€‚
    strict=True æ—¶å‘ç°ä¸¥é‡é”™è¯¯ä¼š raiseï¼›å¦åˆ™ä»…æ‰“å°è­¦å‘Šã€‚
    """
    if df is None or df.empty:
        raise ValueError("æ•°æ®ä¸ºç©º")

    # æ£€æŸ¥ç¼ºå¤±å€¼
    if df.isnull().values.any():
        msg = "è­¦å‘Šï¼šå‘ç°ç¼ºå¤±æ•°æ®"
        if strict:
            raise ValueError(msg)
        if hasattr(validate_data, "_warned_null"):
            pass
        else:
            print(msg)

    # å¿…éœ€åˆ—
    required = ['Open', 'High', 'Low', 'Close', 'Volume']
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing}")

    # æ£€æŸ¥é€»è¾‘é”™è¯¯ï¼šHigh < Low
    if (df['High'] < df['Low']).any():
        raise ValueError("æ•°æ®é”™è¯¯ï¼šå­˜åœ¨ High < Low çš„è¡Œæƒ…")

    # æ£€æŸ¥ Close æ˜¯å¦åœ¨ [Low, High] å†…
    if (df['Close'] > df['High']).any() or (df['Close'] < df['Low']).any():
        raise ValueError("æ•°æ®é”™è¯¯ï¼šClose è¶…å‡º High/Low èŒƒå›´")

    # æ£€æŸ¥åœç‰Œï¼ˆæˆäº¤é‡ä¸º 0ï¼‰ï¼šä»…è­¦å‘Š
    zero_vol = (df['Volume'] == 0).sum()
    if zero_vol > 0:
        print(f"è­¦å‘Šï¼šå‘ç° {zero_vol} è¡Œæˆäº¤é‡ä¸º 0ï¼ˆå¯èƒ½åœç‰Œï¼‰")

    # æ£€æŸ¥å…¨ 0 è¡Œï¼ˆæŸå¤©æ•°æ®å…¨ä¸º 0ï¼‰
    ohlc_zero = ((df['Open'] == 0) & (df['High'] == 0) & (df['Low'] == 0) & (df['Close'] == 0))
    if ohlc_zero.any():
        if strict:
            raise ValueError("æ•°æ®é”™è¯¯ï¼šå­˜åœ¨ OHLC å…¨ä¸º 0 çš„è¡Œæƒ…")
        print("è­¦å‘Šï¼šå­˜åœ¨ OHLC å…¨ä¸º 0 çš„è¡Œæƒ…")

    return True


def add_csv_feed(cerebro, filepath, name, start, end, logger=None):
    """
    è¯»å–å•åªè‚¡ç¥¨ CSVï¼Œè½¬æ¢ä¸º PandasData å¹¶åŠ å…¥ cerebroã€‚
    å…¼å®¹æ ¼å¼ï¼šskiprows=3, åˆ—ä¸º Date, Close, High, Low, Open, Volumeã€‚
    """
    try:
        df = pd.read_csv(
            filepath,
            skiprows=3,
            header=None,
            names=['Date', 'Close', 'High', 'Low', 'Open', 'Volume'],
            parse_dates=[0],
            index_col=0
        )
        for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])
        df = df[(df.index >= pd.Timestamp(start)) & (df.index <= pd.Timestamp(end))]
        if len(df) == 0:
            return False
        validate_data(df, strict=True)
        data = bt.feeds.PandasData(
            dataname=df,
            name=name,
            fromdate=start,
            todate=end,
            open='Open', high='High', low='Low', close='Close', volume='Volume',
            openinterest=None
        )
        cerebro.adddata(data)
        return True
    except Exception as e:
        if logger:
            logger.warning(f"åŠ è½½ {name} å¤±è´¥: {e}")
        return False


def load_data_into_cerebro(cerebro, data_dir, from_date, to_date, universe_size=None, universe_seed=None, logger=None):
    """
    å°† data_dir ä¸‹çš„ CSV åŠ è½½åˆ° cerebroï¼šSPY ä½œä¸º data0ï¼Œå…¶ä½™æŒ‰ universe_size é™åˆ¶æ•°é‡ã€‚
    universe_seed æœ‰å€¼æ—¶ï¼Œå¯¹è‚¡ç¥¨åˆ—è¡¨æ’åºåæŒ‰ç§å­ shuffle å†å–å‰ Nï¼Œä¿è¯å¯å¤ç°ã€‚
    """
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    all_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.csv')])
    if 'SPY.csv' in all_files:
        add_csv_feed(cerebro, os.path.join(data_dir, 'SPY.csv'), 'SPY', from_date, to_date, logger)
        all_files.remove('SPY.csv')
    else:
        if logger:
            logger.warning("æœªæ‰¾åˆ° SPY.csvï¼Œå¤§ç›˜é£æ§å¯èƒ½å¤±æ•ˆ")
    if universe_size is not None and universe_size > 0:
        if universe_seed is not None:
            import random
            rng = random.Random(universe_seed)
            all_files = all_files.copy()
            rng.shuffle(all_files)
        target_files = all_files[:universe_size]
    else:
        target_files = all_files
    for filename in target_files:
        ticker = filename.split('.')[0]
        filepath = os.path.join(data_dir, filename)
        add_csv_feed(cerebro, filepath, ticker, from_date, to_date, logger)
    if logger:
        logger.info(f"ğŸ“Š [æ•°æ®] è£…è½½å®Œæˆã€‚æ€»è®¡: {len(cerebro.datas)} åª (å«SPY)")
    return len(cerebro.datas)


def load_fundamentals(data_dir, logger=None):
    """
    ä» data_dir/fundamentals.csv åŠ è½½åŸºæœ¬é¢æ•°æ®ï¼ˆå¯é€‰ï¼‰ã€‚
    CSV æ ¼å¼ï¼šTicker, PE, PB, ROE, RevenueGrowth, DebtToEquity
    - PE/PB: å¸‚ç›ˆç‡/å¸‚å‡€ç‡ï¼Œç©ºæˆ–è´Ÿè¡¨ç¤ºäºæŸæˆ–æ— æ•ˆï¼Œè¿‡æ»¤æ—¶å¯è·³è¿‡
    - ROE/RevenueGrowth: å°æ•°å½¢å¼ï¼Œå¦‚ 0.15 è¡¨ç¤º 15%
    - DebtToEquity: è´Ÿå€º/æƒç›Šï¼Œç©ºåˆ™ä¸è¿‡æ»¤
    è¿”å›: DataFrame index=Ticker, columns=pe,pb,roe,revenue_growth,debt_to_equityï¼›æ— æ–‡ä»¶æˆ–å¤±è´¥è¿”å› Noneã€‚
    """
    if not data_dir or not os.path.exists(data_dir):
        return None
    path = os.path.join(data_dir, 'fundamentals.csv')
    if not os.path.exists(path):
        return None
    try:
        df = pd.read_csv(path, dtype=str)
        df = df.rename(columns=lambda c: c.strip().lower().replace(' ', '_'))
        col_map = {'ticker': 'Ticker', 'pe': 'PE', 'pb': 'PB', 'roe': 'ROE',
                   'revenue_growth': 'RevenueGrowth', 'revenuegrowth': 'RevenueGrowth',
                   'debt_to_equity': 'DebtToEquity', 'debttoequity': 'DebtToEquity'}
        for k, v in col_map.items():
            if k in df.columns and v not in df.columns:
                df[v] = df[k]
        if 'Ticker' not in df.columns and 'ticker' in df.columns:
            df['Ticker'] = df['ticker']
        numeric_cols = ['PE', 'PB', 'ROE', 'RevenueGrowth', 'DebtToEquity']
        for c in numeric_cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors='coerce')
        df = df.set_index('Ticker')
        keep = [c for c in numeric_cols if c in df.columns]
        df = df[keep] if keep else df
        if logger:
            logger.debug(f"åŸºæœ¬é¢æ•°æ®å·²åŠ è½½: {path}, {len(df)} åª")
        return df
    except Exception as e:
        if logger:
            logger.warning(f"åŠ è½½åŸºæœ¬é¢æ–‡ä»¶å¤±è´¥ {path}: {e}")
        return None
